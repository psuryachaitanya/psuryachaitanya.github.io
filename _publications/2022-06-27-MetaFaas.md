---
title: "MetaFaaS: learning-to-learn on serverless"
collection: publications
permalink: /publication/Meta-FaaS
excerpt: 'Meta-learning is a technique to transfer learning from a pre-built model on known tasks to build a model for unknown tasks. Graident-based meta-learning algorithms are one such family that use the technique of gradient descent for model updates. These meta-learning architectures are hierarchical in nature and hence incur large training times, which are prohibitive for industries relying on models trained using the most recent data to make relevant predictions. To address these issues, we propose MetaFaaS, a function-as-a-service (FaaS) paradigm on public cloud to build a scalable and cost-performance optimal deployment framework for gradient-based meta-learning architectures. We propose an analytical model to predict the cost and training time on cloud for a given workload. We validate our approach on multiple meta-learning architectures, (MAML, ANIL, ALFA) and attain a speed-up of over 5× in training time on FaaS. We also propose eALFA, a compute-efficient meta-learning architecture, which achieves a speed-up of > 9× as compared to ALFA. We present our results with four quasi-benchmark datasets in meta-learning, namely, Omniglot, Mini-Imagenet (Imagenet), FC100 (CIFAR), and CUBirds200.'
date: 2022-06-12
venue: "BiDEDE '22: Proceedings of The International Workshop on Big Data in Emergent Distributed Environments"
paperurl: 'https://doi.org/10.1145/3530050.3532926'
citation: "Varad Pimpalkhute, Shruti Kunde, Rekha Singhal, Surya Palepu, Dheeraj Chahal, and Amey Pandit. 2022. MetaFaaS: learning-to-learn on serverless. In Proceedings of The International Workshop on Big Data in Emergent Distributed Environments (BiDEDE '22). Association for Computing Machinery, New York, NY, USA, Article 1, 1–9."
---

## Abstract
Meta-learning is a technique to transfer learning from a pre-built model on known tasks to build a model for unknown tasks. Graident-based meta-learning algorithms are one such family that use the technique of gradient descent for model updates. These meta-learning architectures are hierarchical in nature and hence incur large training times, which are prohibitive for industries relying on models trained using the most recent data to make relevant predictions. To address these issues, we propose MetaFaaS, a function-as-a-service (FaaS) paradigm on public cloud to build a scalable and cost-performance optimal deployment framework for gradient-based meta-learning architectures. We propose an analytical model to predict the cost and training time on cloud for a given workload. We validate our approach on multiple meta-learning architectures, (MAML, ANIL, ALFA) and attain a speed-up of over 5× in training time on FaaS. We also propose eALFA, a compute-efficient meta-learning architecture, which achieves a speed-up of > 9× as compared to ALFA. We present our results with four quasi-benchmark datasets in meta-learning, namely, Omniglot, Mini-Imagenet (Imagenet), FC100 (CIFAR), and CUBirds200.